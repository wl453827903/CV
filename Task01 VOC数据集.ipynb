{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VOC数据集的dataloader的构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"python\n",
    "    create_data_lists\n",
    "\"\"\"\n",
    "from utils import create_data_lists\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # voc07_path，voc12_path为我们训练测试所需要用到的数据集，output_folder为我们生成构建dataloader所需文件的路径\n",
    "    # 参数中涉及的路径以个人实际路径为准，建议将数据集放到dataset目录下，和教程保持一致\n",
    "    create_data_lists(voc07_path='../../../dataset/VOCdevkit/VOC2007',\n",
    "                      voc12_path='../../../dataset/VOCdevkit/VOC2012',\n",
    "                      output_folder='../../../dataset/VOCdevkit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"python\n",
    "    xml文件解析\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import xml.etree.ElementTree as ET    #解析xml文件所用工具\n",
    "import torchvision.transforms.functional as FT\n",
    "\n",
    "#GPU设置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Label map\n",
    "#voc_labels为VOC数据集中20类目标的类别名称\n",
    "voc_labels = ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable',\n",
    "              'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor')\n",
    "\n",
    "#创建label_map字典，用于存储类别和类别索引之间的映射关系。比如：{1：'aeroplane'， 2：'bicycle'，......}\n",
    "label_map = {k: v + 1 for v, k in enumerate(voc_labels)}\n",
    "#VOC数据集默认不含有20类目标中的其中一类的图片的类别为background，类别索引设置为0\n",
    "label_map['background'] = 0\n",
    "\n",
    "#将映射关系倒过来，{类别名称：类别索引}\n",
    "rev_label_map = {v: k for k, v in label_map.items()}  # Inverse mapping\n",
    "\n",
    "#解析xml文件，最终返回这张图片中所有目标的标注框及其类别信息，以及这个目标是否是一个difficult目标\n",
    "def parse_annotation(annotation_path):\n",
    "    #解析xml\n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    boxes = list()    #存储bbox\n",
    "    labels = list()    #存储bbox对应的label\n",
    "    difficulties = list()    #存储bbox对应的difficult信息\n",
    "\n",
    "    #遍历xml文件中所有的object，前面说了，有多少个object就有多少个目标\n",
    "    for object in root.iter('object'):\n",
    "        #提取每个object的difficult、label、bbox信息\n",
    "        difficult = int(object.find('difficult').text == '1')\n",
    "        label = object.find('name').text.lower().strip()\n",
    "        if label not in label_map:\n",
    "            continue\n",
    "        bbox = object.find('bndbox')\n",
    "        xmin = int(bbox.find('xmin').text) - 1\n",
    "        ymin = int(bbox.find('ymin').text) - 1\n",
    "        xmax = int(bbox.find('xmax').text) - 1\n",
    "        ymax = int(bbox.find('ymax').text) - 1\n",
    "        #存储\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "        labels.append(label_map[label])\n",
    "        difficulties.append(difficult)\n",
    "\n",
    "    #返回包含图片标注信息的字典\n",
    "    return {'boxes': boxes, 'labels': labels, 'difficulties': difficulties}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"python\n",
    "    分别读取train和valid的图片和xml信息，创建用于训练和测试的json文件\n",
    "\"\"\"\n",
    "def create_data_lists(voc07_path, voc12_path, output_folder):\n",
    "    \"\"\"\n",
    "    Create lists of images, the bounding boxes and labels of the objects in these images, and save these to file.\n",
    "    :param voc07_path: path to the 'VOC2007' folder\n",
    "    :param voc12_path: path to the 'VOC2012' folder\n",
    "    :param output_folder: folder where the JSONs must be saved\n",
    "    \"\"\"\n",
    "\n",
    "    #获取voc2007和voc2012数据集的绝对路径\n",
    "    voc07_path = os.path.abspath(voc07_path)\n",
    "    voc12_path = os.path.abspath(voc12_path)\n",
    "\n",
    "    train_images = list()\n",
    "    train_objects = list()\n",
    "    n_objects = 0\n",
    "\n",
    "    # Training data\n",
    "    for path in [voc07_path, voc12_path]:\n",
    "\n",
    "        # Find IDs of images in training data\n",
    "        #获取训练所用的train和val数据的图片id\n",
    "        with open(os.path.join(path, 'ImageSets/Main/trainval.txt')) as f:\n",
    "            ids = f.read().splitlines()\n",
    "\n",
    "        #根据图片id，解析图片的xml文件，获取标注信息\n",
    "        for id in ids:\n",
    "            # Parse annotation's XML file\n",
    "            objects = parse_annotation(os.path.join(path, 'Annotations', id + '.xml'))\n",
    "            if len(objects['boxes']) == 0:    #如果没有目标则跳过\n",
    "                continue\n",
    "            n_objects += len(objects)        #统计目标总数\n",
    "            train_objects.append(objects)    #存储每张图片的标注信息到列表train_objects\n",
    "            train_images.append(os.path.join(path, 'JPEGImages', id + '.jpg'))    #存储每张图片的路径到列表train_images，用于读取图片\n",
    "\n",
    "    assert len(train_objects) == len(train_images)        #检查图片数量和标注信息量是否相等，相等才继续执行程序\n",
    "\n",
    "    # Save to file\n",
    "    #将训练数据的图片路径，标注信息，类别映射信息，分别保存为json文件\n",
    "    with open(os.path.join(output_folder, 'TRAIN_images.json'), 'w') as j:\n",
    "        json.dump(train_images, j)\n",
    "    with open(os.path.join(output_folder, 'TRAIN_objects.json'), 'w') as j:\n",
    "        json.dump(train_objects, j)\n",
    "    with open(os.path.join(output_folder, 'label_map.json'), 'w') as j:\n",
    "        json.dump(label_map, j)  # save label map too\n",
    "\n",
    "    print('\\nThere are %d training images containing a total of %d objects. Files have been saved to %s.' % (\n",
    "        len(train_images), n_objects, os.path.abspath(output_folder)))\n",
    "\n",
    "\n",
    "    #与Train data一样，目的是将测试数据的图片路径，标注信息，类别映射信息，分别保存为json文件，参考上面的注释理解\n",
    "    # Test data\n",
    "    test_images = list()\n",
    "    test_objects = list()\n",
    "    n_objects = 0\n",
    "\n",
    "    # Find IDs of images in the test data\n",
    "    with open(os.path.join(voc07_path, 'ImageSets/Main/test.txt')) as f:\n",
    "        ids = f.read().splitlines()\n",
    "\n",
    "    for id in ids:\n",
    "        # Parse annotation's XML file\n",
    "        objects = parse_annotation(os.path.join(voc07_path, 'Annotations', id + '.xml'))\n",
    "        if len(objects) == 0:\n",
    "            continue\n",
    "        test_objects.append(objects)\n",
    "        n_objects += len(objects)\n",
    "        test_images.append(os.path.join(voc07_path, 'JPEGImages', id + '.jpg'))\n",
    "\n",
    "    assert len(test_objects) == len(test_images)\n",
    "\n",
    "    # Save to file\n",
    "    with open(os.path.join(output_folder, 'TEST_images.json'), 'w') as j:\n",
    "        json.dump(test_images, j)\n",
    "    with open(os.path.join(output_folder, 'TEST_objects.json'), 'w') as j:\n",
    "        json.dump(test_objects, j)\n",
    "\n",
    "    print('\\nThere are %d test images containing a total of %d objects. Files have been saved to %s.' % (\n",
    "        len(test_images), n_objects, os.path.abspath(output_folder)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #train_dataset和train_loader的实例化\n",
    "    train_dataset = PascalVOCDataset(data_folder,\n",
    "                                     split='train',\n",
    "                                     keep_difficult=keep_difficult)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                               collate_fn=train_dataset.collate_fn, num_workers=workers,\n",
    "                                               pin_memory=True)  # note that we're passing the collate function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"python\n",
    "    PascalVOCDataset具体实现过程\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from utils import transform\n",
    "\n",
    "\n",
    "class PascalVOCDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n",
    "    \"\"\"\n",
    "\n",
    "    #初始化相关变量\n",
    "    #读取images和objects标注信息\n",
    "    def __init__(self, data_folder, split, keep_difficult=False):\n",
    "        \"\"\"\n",
    "        :param data_folder: folder where data files are stored\n",
    "        :param split: split, one of 'TRAIN' or 'TEST'\n",
    "        :param keep_difficult: keep or discard objects that are considered difficult to detect?\n",
    "        \"\"\"\n",
    "        self.split = split.upper()    #保证输入为纯大写字母，便于匹配{'TRAIN', 'TEST'}\n",
    "\n",
    "        assert self.split in {'TRAIN', 'TEST'}\n",
    "\n",
    "        self.data_folder = data_folder\n",
    "        self.keep_difficult = keep_difficult\n",
    "\n",
    "        # Read data files\n",
    "        with open(os.path.join(data_folder, self.split + '_images.json'), 'r') as j:\n",
    "            self.images = json.load(j)\n",
    "        with open(os.path.join(data_folder, self.split + '_objects.json'), 'r') as j:\n",
    "            self.objects = json.load(j)\n",
    "\n",
    "        assert len(self.images) == len(self.objects)\n",
    "\n",
    "    #循环读取image及对应objects\n",
    "    #对读取的image及objects进行tranform操作（数据增广）\n",
    "    #返回PIL格式图像，标注框，标注框对应的类别索引，对应的difficult标志(True or False)\n",
    "    def __getitem__(self, i):\n",
    "        # Read image\n",
    "        #*需要注意，在pytorch中，图像的读取要使用Image.open()读取成PIL格式，不能使用opencv\n",
    "        #*由于Image.open()读取的图片是四通道的(RGBA)，因此需要.convert('RGB')转换为RGB通道\n",
    "        image = Image.open(self.images[i], mode='r')\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "        # Read objects in this image (bounding boxes, labels, difficulties)\n",
    "        objects = self.objects[i]\n",
    "        boxes = torch.FloatTensor(objects['boxes'])  # (n_objects, 4)\n",
    "        labels = torch.LongTensor(objects['labels'])  # (n_objects)\n",
    "        difficulties = torch.ByteTensor(objects['difficulties'])  # (n_objects)\n",
    "\n",
    "        # Discard difficult objects, if desired\n",
    "        #如果self.keep_difficult为False,即不保留difficult标志为True的目标\n",
    "        #那么这里将对应的目标删去\n",
    "        if not self.keep_difficult:\n",
    "            boxes = boxes[1 - difficulties]\n",
    "            labels = labels[1 - difficulties]\n",
    "            difficulties = difficulties[1 - difficulties]\n",
    "\n",
    "        # Apply transformations\n",
    "        #对读取的图片应用transform\n",
    "        image, boxes, labels, difficulties = transform(image, boxes, labels, difficulties, split=self.split)\n",
    "\n",
    "        return image, boxes, labels, difficulties\n",
    "\n",
    "    #获取图片的总数，用于计算batch数\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    #我们知道，我们输入到网络中训练的数据通常是一个batch一起输入，而通过__getitem__我们只读取了一张图片及其objects信息\n",
    "    #如何将读取的一张张图片及其object信息整合成batch的形式呢？\n",
    "    #collate_fn就是做这个事情，\n",
    "    #对于一个batch的images，collate_fn通过torch.stack()将其整合成4维tensor，对应的objects信息分别用一个list存储\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Since each image may have a different number of objects, we need a collate function (to be passed to the DataLoader).\n",
    "        This describes how to combine these tensors of different sizes. We use lists.\n",
    "        Note: this need not be defined in this Class, can be standalone.\n",
    "        :param batch: an iterable of N sets from __getitem__()\n",
    "        :return: a tensor of images, lists of varying-size tensors of bounding boxes, labels, and difficulties\n",
    "        \"\"\"\n",
    "\n",
    "        images = list()\n",
    "        boxes = list()\n",
    "        labels = list()\n",
    "        difficulties = list()\n",
    "\n",
    "        for b in batch:\n",
    "            images.append(b[0])\n",
    "            boxes.append(b[1])\n",
    "            labels.append(b[2])\n",
    "            difficulties.append(b[3])\n",
    "\n",
    "        #(3,224,224) -> (N,3,224,224)\n",
    "        images = torch.stack(images, dim=0)\n",
    "\n",
    "        return images, boxes, labels, difficulties  # tensor (N, 3, 224, 224), 3 lists of N tensors each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"python\n",
    "    transform操作是训练模型中一项非常重要的工作，其中不仅包含数据增强以提升模型性能的相关操作，也包含如数据类型转换(PIL to Tensor)、归一化(Normalize)这些必要操作。\n",
    "\"\"\"\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "import torchvision.transforms.functional as FT\n",
    "\n",
    "\"\"\"\n",
    "可以看到，transform分为TRAIN和TEST两种模式，以本实验为例：\n",
    "\n",
    "在TRAIN时进行的transform有：\n",
    "1.以随机顺序改变图片亮度，对比度，饱和度和色相，每种都有50％的概率被执行。photometric_distort\n",
    "2.扩大目标，expand\n",
    "3.随机裁剪图片，random_crop\n",
    "4.0.5的概率进行图片翻转，flip\n",
    "*注意：a. 第一种transform属于像素级别的图像增强，目标相对于图片的位置没有改变，因此bbox坐标不需要变化。\n",
    "         但是2，3，4，5都属于图片的几何变化，目标相对于图片的位置被改变，因此bbox坐标要进行相应变化。\n",
    "\n",
    "在TRAIN和TEST时都要进行的transform有：\n",
    "1.统一图像大小到(224,224)，resize\n",
    "2.PIL to Tensor\n",
    "3.归一化，FT.normalize()\n",
    "\n",
    "注1: resize也是一种几何变化，要知道应用数据增强策略时，哪些属于几何变化，哪些属于像素变化\n",
    "注2: PIL to Tensor操作，normalize操作必须执行\n",
    "\"\"\"\n",
    "\n",
    "def transform(image, boxes, labels, difficulties, split):\n",
    "    \"\"\"\n",
    "    Apply the transformations above.\n",
    "    :param image: image, a PIL Image\n",
    "    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n",
    "    :param labels: labels of objects, a tensor of dimensions (n_objects)\n",
    "    :param difficulties: difficulties of detection of these objects, a tensor of dimensions (n_objects)\n",
    "    :param split: one of 'TRAIN' or 'TEST', since different sets of transformations are applied\n",
    "    :return: transformed image, transformed bounding box coordinates, transformed labels, transformed difficulties\n",
    "    \"\"\"\n",
    "\n",
    "    #在训练和测试时使用的transform策略往往不完全相同，所以需要split变量指明是TRAIN还是TEST时的transform方法\n",
    "    assert split in {'TRAIN', 'TEST'}\n",
    "\n",
    "    # Mean and standard deviation of ImageNet data that our base VGG from torchvision was trained on\n",
    "    # see: https://pytorch.org/docs/stable/torchvision/models.html\n",
    "    #为了防止由于图片之间像素差异过大而导致的训练不稳定问题，图片在送入网络训练之间需要进行归一化\n",
    "    #对所有图片各通道求mean和std来获得\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    new_image = image\n",
    "    new_boxes = boxes\n",
    "    new_labels = labels\n",
    "    new_difficulties = difficulties\n",
    "\n",
    "    # Skip the following operations for evaluation/testing\n",
    "    if split == 'TRAIN':\n",
    "        # A series of photometric distortions in random order, each with 50% chance of occurrence, as in Caffe repo\n",
    "        new_image = photometric_distort(new_image)\n",
    "\n",
    "        # Convert PIL image to Torch tensor\n",
    "        new_image = FT.to_tensor(new_image)\n",
    "\n",
    "        # Expand image (zoom out) with a 50% chance - helpful for training detection of small objects\n",
    "        # Fill surrounding space with the mean of ImageNet data that our base VGG was trained on\n",
    "        if random.random() < 0.5:\n",
    "            new_image, new_boxes = expand(new_image, boxes, filler=mean)\n",
    "\n",
    "        # Randomly crop image (zoom in)\n",
    "        new_image, new_boxes, new_labels, new_difficulties = random_crop(new_image, new_boxes, new_labels,\n",
    "                                                                         new_difficulties)\n",
    "\n",
    "        # Convert Torch tensor to PIL image\n",
    "        new_image = FT.to_pil_image(new_image)\n",
    "\n",
    "        # Flip image with a 50% chance\n",
    "        if random.random() < 0.5:\n",
    "            new_image, new_boxes = flip(new_image, new_boxes)\n",
    "\n",
    "    # Resize image to (224, 224) - this also converts absolute boundary coordinates to their fractional form\n",
    "    new_image, new_boxes = resize(new_image, new_boxes, dims=(224, 224))\n",
    "\n",
    "    # Convert PIL image to Torch tensor\n",
    "    new_image = FT.to_tensor(new_image)\n",
    "\n",
    "    # Normalize by mean and standard deviation of ImageNet data that our base VGG was trained on\n",
    "    new_image = FT.normalize(new_image, mean=mean, std=std)\n",
    "\n",
    "    return new_image, new_boxes, new_labels, new_difficulties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"python\n",
    "    DataLoader\n",
    "\"\"\"\n",
    "#参数说明：\n",
    "#在train时一般设置shufle=True打乱数据顺序，增强模型的鲁棒性\n",
    "#num_worker表示读取数据时的线程数，一般根据自己设备配置确定（如果是windows系统，建议设默认值0，防止出错）\n",
    "#pin_memory，在计算机内存充足的时候设置为True可以加快内存中的tensor转换到GPU的速度，具体原因可以百度哈~\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                           collate_fn=train_dataset.collate_fn, num_workers=workers,\n",
    "                                           pin_memory=True)  # note that we're passing the collate function here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
