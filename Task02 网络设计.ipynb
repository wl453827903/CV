{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 锚框 or 先验框"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "设置细节介绍：\n",
    "1. 离散程度 fmap_dims = 7： VGG16最后的特征图尺寸为 7*7\n",
    "2. 在上面的举例中我们是假设了三种尺寸的先验框，然后遍历坐标。在先验框生成过程中，先验框的尺寸是提前设置好的，\n",
    "   本教程为特征图上每一个cell定义了共9种不同大小和形状的候选框（3种尺度*3种长宽比=9）\n",
    "\n",
    "生成过程：\n",
    "0. cx， cy表示中心点坐标\n",
    "1. 遍历特征图上每一个cell，i+0.5是为了从坐标点移动至cell中心，/fmap_dims目的是将坐标在特征图上归一化\n",
    "2. 这个时候我们已经可以在每个cell上各生成一个框了，但是这个不是我们需要的，我们称之为base_prior_bbox基准框。\n",
    "3. 根据我们在每个cell上得到的长宽比1:1的基准框，结合我们设置的3种尺度obj_scales和3种长宽比aspect_ratios就得到了每个cell的9个先验框。\n",
    "4. 最终结果保存在prior_boxes中并返回。\n",
    "\n",
    "需要注意的是，这个时候我们的到的先验框是针对特征图的尺寸并归一化的，因此要映射到原图计算IOU或者展示，需要：\n",
    "img_prior_boxes = prior_boxes * 图像尺寸\n",
    "\"\"\"\n",
    "\n",
    "def create_prior_boxes():\n",
    "        \"\"\"\n",
    "        Create the 441 prior (default) boxes for the network, as described in the tutorial.\n",
    "        VGG16最后的特征图尺寸为 7*7\n",
    "        我们为特征图上每一个cell定义了共9种不同大小和形状的候选框（3种尺度*3种长宽比=9）\n",
    "        因此总的候选框个数 = 7 * 7 * 9 = 441\n",
    "        :return: prior boxes in center-size coordinates, a tensor of dimensions (441, 4)\n",
    "        \"\"\"\n",
    "        fmap_dims = 7 \n",
    "        obj_scales = [0.2, 0.4, 0.6]\n",
    "        aspect_ratios = [1., 2., 0.5]\n",
    "\n",
    "        prior_boxes = []\n",
    "        for i in range(fmap_dims):\n",
    "            for j in range(fmap_dims):\n",
    "                cx = (j + 0.5) / fmap_dims\n",
    "                cy = (i + 0.5) / fmap_dims\n",
    "\n",
    "                for obj_scale in obj_scales:\n",
    "                    for ratio in aspect_ratios:\n",
    "                        prior_boxes.append([cx, cy, obj_scale * sqrt(ratio), obj_scale / sqrt(ratio)])\n",
    "\n",
    "        prior_boxes = torch.FloatTensor(prior_boxes).to(device)  # (441, 4)\n",
    "        prior_boxes.clamp_(0, 1)  # (441, 4)\n",
    "\n",
    "        return prior_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGBase(nn.Module):                                                                                                                                         \n",
    "    \"\"\"\n",
    "    VGG base convolutions to produce feature maps.\n",
    "    完全采用vgg16的结构作为特征提取模块，丢掉fc6和fc7两个全连接层。\n",
    "    因为vgg16的ImageNet预训练模型是使用224×224尺寸训练的，因此我们的网络输入也固定为224×224\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VGGBase, self).__init__()\n",
    "\n",
    "        # Standard convolutional layers in VGG16\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)  # stride = 1, by default\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)    # 224->112\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)    # 112->56\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)    # 56->28\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)    # 28->14\n",
    "\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)    # 14->7\n",
    "\n",
    "        # Load pretrained weights on ImageNet\n",
    "        self.load_pretrained_layers()\n",
    "\n",
    "\n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        :param image: images, a tensor of dimensions (N, 3, 224, 224)\n",
    "        :return: feature maps pool5\n",
    "        \"\"\"\n",
    "        out = F.relu(self.conv1_1(image))  # (N, 64, 224, 224)\n",
    "        out = F.relu(self.conv1_2(out))  # (N, 64, 224, 224)\n",
    "        out = self.pool1(out)  # (N, 64, 112, 112)\n",
    "\n",
    "        out = F.relu(self.conv2_1(out))  # (N, 128, 112, 112)\n",
    "        out = F.relu(self.conv2_2(out))  # (N, 128, 112, 112)\n",
    "        out = self.pool2(out)  # (N, 128, 56, 56)\n",
    "\n",
    "        out = F.relu(self.conv3_1(out))  # (N, 256, 56, 56)\n",
    "        out = F.relu(self.conv3_2(out))  # (N, 256, 56, 56)\n",
    "        out = F.relu(self.conv3_3(out))  # (N, 256, 56, 56)\n",
    "        out = self.pool3(out)  # (N, 256, 28, 28)\n",
    "\n",
    "        out = F.relu(self.conv4_1(out))  # (N, 512, 28, 28)\n",
    "        out = F.relu(self.conv4_2(out))  # (N, 512, 28, 28)\n",
    "        out = F.relu(self.conv4_3(out))  # (N, 512, 28, 28)\n",
    "        out = self.pool4(out)  # (N, 512, 14, 14)\n",
    "\n",
    "        out = F.relu(self.conv5_1(out))  # (N, 512, 14, 14)\n",
    "        out = F.relu(self.conv5_2(out))  # (N, 512, 14, 14)\n",
    "        out = F.relu(self.conv5_3(out))  # (N, 512, 14, 14)\n",
    "        out = self.pool5(out)  # (N, 512, 7, 7)\n",
    "\n",
    "        # return 7*7 feature map                                                                                                                                  \n",
    "        return out\n",
    "\n",
    "\n",
    "    def load_pretrained_layers(self):\n",
    "        \"\"\"\n",
    "        we use a VGG-16 pretrained on the ImageNet task as the base network.\n",
    "        There's one available in PyTorch, see https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.vgg16\n",
    "        We copy these parameters into our network. It's straightforward for conv1 to conv5.\n",
    "        \"\"\"\n",
    "        # Current state of base\n",
    "        state_dict = self.state_dict()\n",
    "        param_names = list(state_dict.keys())\n",
    "\n",
    "        # Pretrained VGG base\n",
    "        pretrained_state_dict = torchvision.models.vgg16(pretrained=True).state_dict()\n",
    "        pretrained_param_names = list(pretrained_state_dict.keys())\n",
    "\n",
    "        # Transfer conv. parameters from pretrained model to current model\n",
    "        for i, param in enumerate(param_names):  \n",
    "            state_dict[param] = pretrained_state_dict[pretrained_param_names[i]]\n",
    "\n",
    "        self.load_state_dict(state_dict)\n",
    "        print(\"\\nLoaded base model.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类头和回归头"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 边界框的编解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cxcy_to_gcxgcy(cxcy, priors_cxcy):\n",
    "    \"\"\" \n",
    "    Encode bounding boxes (that are in center-size form) w.r.t. the corresponding prior boxes (that are in center-size form).\n",
    "\n",
    "    For the center coordinates, find the offset with respect to the prior box, and scale by the size of the prior box.\n",
    "    For the size coordinates, scale by the size of the prior box, and convert to the log-space.\n",
    "\n",
    "    In the model, we are predicting bounding box coordinates in this encoded form.\n",
    "\n",
    "    :param cxcy: bounding boxes in center-size coordinates, a tensor of size (n_priors, 4)\n",
    "    :param priors_cxcy: prior boxes with respect to which the encoding must be performed, a tensor of size (n_priors, 4)\n",
    "    :return: encoded bounding boxes, a tensor of size (n_priors, 4)\n",
    "    \"\"\"\n",
    "\n",
    "    # The 10 and 5 below are referred to as 'variances' in the original SSD Caffe repo, completely empirical\n",
    "    # They are for some sort of numerical conditioning, for 'scaling the localization gradient'\n",
    "    # See https://github.com/weiliu89/caffe/issues/155\n",
    "    return torch.cat([(cxcy[:, :2] - priors_cxcy[:, :2]) / (priors_cxcy[:, 2:] / 10),  # g_c_x, g_c_y\n",
    "                      torch.log(cxcy[:, 2:] / priors_cxcy[:, 2:]) * 5], 1)  # g_w, g_h\n",
    "\n",
    "\n",
    "def gcxgcy_to_cxcy(gcxgcy, priors_cxcy):\n",
    "    \"\"\" \n",
    "    Decode bounding box coordinates predicted by the model, since they are encoded in the form mentioned above.\n",
    "\n",
    "    They are decoded into center-size coordinates.\n",
    "\n",
    "    This is the inverse of the function above.\n",
    "\n",
    "    :param gcxgcy: encoded bounding boxes, i.e. output of the model, a tensor of size (n_priors, 4)\n",
    "    :param priors_cxcy: prior boxes with respect to which the encoding is defined, a tensor of size (n_priors, 4)\n",
    "    :return: decoded bounding boxes in center-size form, a tensor of size (n_priors, 4)\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.cat([gcxgcy[:, :2] * priors_cxcy[:, 2:] / 10 + priors_cxcy[:, :2],  # c_x, c_y\n",
    "                      torch.exp(gcxgcy[:, 2:] / 5) * priors_cxcy[:, 2:]], 1)  # w, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分类头与回归头预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionConvolutions(nn.Module):\n",
    "    \"\"\" \n",
    "    Convolutions to predict class scores and bounding boxes using feature maps.\n",
    "\n",
    "    The bounding boxes (locations) are predicted as encoded offsets w.r.t each of the 441 prior (default) boxes.\n",
    "    See 'cxcy_to_gcxgcy' in utils.py for the encoding definition.\n",
    "    这里预测坐标的编码方式完全遵循的SSD的定义\n",
    "\n",
    "    The class scores represent the scores of each object class in each of the 441 bounding boxes located.\n",
    "    A high score for 'background' = no object.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        \"\"\" \n",
    "        :param n_classes: number of different types of objects\n",
    "        \"\"\"\n",
    "        super(PredictionConvolutions, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # Number of prior-boxes we are considering per position in the feature map\n",
    "        # 9 prior-boxes implies we use 9 different aspect ratios, etc.\n",
    "        n_boxes = 9 \n",
    "\n",
    "        # Localization prediction convolutions (predict offsets w.r.t prior-boxes)\n",
    "        self.loc_conv = nn.Conv2d(512, n_boxes * 4, kernel_size=3, padding=1)\n",
    "\n",
    "        # Class prediction convolutions (predict classes in localization boxes)\n",
    "        self.cl_conv = nn.Conv2d(512, n_boxes * n_classes, kernel_size=3, padding=1)\n",
    "\n",
    "        # Initialize convolutions' parameters\n",
    "        self.init_conv2d()\n",
    "\n",
    "\n",
    "    def init_conv2d(self):\n",
    "        \"\"\"\n",
    "        Initialize convolution parameters.\n",
    "        \"\"\"\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(c.weight)\n",
    "                nn.init.constant_(c.bias, 0.)\n",
    "\n",
    "\n",
    "    def forward(self, pool5_feats):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        :param pool5_feats: conv4_3 feature map, a tensor of dimensions (N, 512, 7, 7)\n",
    "        :return: 441 locations and class scores (i.e. w.r.t each prior box) for each image\n",
    "        \"\"\"\n",
    "        batch_size = pool5_feats.size(0)\n",
    "\n",
    "        # Predict localization boxes' bounds (as offsets w.r.t prior-boxes)\n",
    "        l_conv = self.loc_conv(pool5_feats)  # (N, n_boxes * 4, 7, 7)\n",
    "        l_conv = l_conv.permute(0, 2, 3, 1).contiguous()  \n",
    "        # (N, 7, 7, n_boxes * 4), to match prior-box order (after .view())\n",
    "        # (.contiguous() ensures it is stored in a contiguous chunk of memory, needed for .view() below)\n",
    "        locs = l_conv.view(batch_size, -1, 4)  # (N, 441, 4), there are a total 441 boxes on this feature map\n",
    "\n",
    "        # Predict classes in localization boxes\n",
    "        c_conv = self.cl_conv(pool5_feats)  # (N, n_boxes * n_classes, 7, 7)\n",
    "        c_conv = c_conv.permute(0, 2, 3, 1).contiguous()  # (N, 7, 7, n_boxes * n_classes), to match prior-box order (after .view())\n",
    "        classes_scores = c_conv.view(batch_size, -1, self.n_classes)  # (N, 441, n_classes), there are a total 441 boxes on this feature map\n",
    "\n",
    "        return locs, classes_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
